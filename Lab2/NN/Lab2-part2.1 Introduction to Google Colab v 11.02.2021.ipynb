{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Lab3-part1 Introduction to Google Colab v 11.02.2021.ipynb","provenance":[{"file_id":"1DCKj5pwt2-YGyvwpP4a50ikWH9SsxtKt","timestamp":1615465982341}],"collapsed_sections":["csu4BjubRBBR","iWzFmbSBRBBV","OwvwBgudUAt7","XCWHC2oLMyJB","2ScCMexzOTx6"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sAI8gIO4RBBC"},"source":["# IS 784 Google Colab and Pytorch Basics"]},{"cell_type":"markdown","metadata":{"id":"ap84W3FeRBBO"},"source":["Pytorch is a Python deep learning library from Facebook mainly written with CUDA, which is a general purpose GPU computing framework of NVIDIA. Its low level design is intended to be similar with Numpy and to follow Python principles as much as possible. It facilitates deep learning applications with its CUDA backend, automatic differentiation mechanism on computational graphs and its utilities for machine learning and deep learning."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQNN8CFoRBBP","executionInfo":{"status":"ok","timestamp":1646354069101,"user_tz":-180,"elapsed":8884,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"34472e74-e731-430d-9bbf-8ee9fc00ced5"},"source":["import torch\n","print(torch.__version__) #version of the pytorh"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"csu4BjubRBBR"},"source":["### Part 1 Basics\n"]},{"cell_type":"markdown","metadata":{"id":"ln5gQ60fUXuR"},"source":["With Pytorch, you can create tensors in the way that you create arrays in Numpy. `torch.Tensor` is the main data structure of Pytorch. It holds the necesarry information for computation of created computational graphs in both forward and backward mode. Each tensor is an edge in computation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhakNZKWRBBR","executionInfo":{"status":"ok","timestamp":1646354099201,"user_tz":-180,"elapsed":269,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"6ef89c49-3831-43fc-cb9f-95d77f477f73"},"source":["# basic data operations \n","x1 = torch.zeros(5, 3) #you can initilize tensors with zeros, ones or rand methods\n","print(x1)\n","print(x1.dtype)\n","print(x1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]])\n","torch.float32\n","torch.Size([5, 3])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMTQHq7cRBBS","executionInfo":{"status":"ok","timestamp":1646354099441,"user_tz":-180,"elapsed":7,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"919f3fe2-62ef-41ef-d4c9-d0017172bac6"},"source":["x2 = torch.ones(5, 3) * 2.5  #example of ones method\n","print(x2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2.5000, 2.5000, 2.5000],\n","        [2.5000, 2.5000, 2.5000],\n","        [2.5000, 2.5000, 2.5000],\n","        [2.5000, 2.5000, 2.5000],\n","        [2.5000, 2.5000, 2.5000]])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tF0QZs9sRBBS","executionInfo":{"status":"ok","timestamp":1646354099441,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"377aa470-307c-4ce0-aeb4-0781496f0cdf"},"source":["x3 = torch.rand(5, 3)  # example of rand method\n","print(x3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.5970, 0.5432, 0.7493],\n","        [0.1905, 0.6017, 0.7007],\n","        [0.6418, 0.4654, 0.0508],\n","        [0.4970, 0.7959, 0.2184],\n","        [0.7858, 0.7914, 0.7779]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"FgF_h6_VRBBS"},"source":[" Also, you can apply mathematical operations on tensors just as other scientific computing libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"074NSHOPRBBT","executionInfo":{"status":"ok","timestamp":1646354099442,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"d440b0d0-2cee-42c9-9d35-db742e86b0da"},"source":["print(x2 + x3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3.0970, 3.0432, 3.2493],\n","        [2.6905, 3.1017, 3.2007],\n","        [3.1418, 2.9654, 2.5508],\n","        [2.9970, 3.2959, 2.7184],\n","        [3.2858, 3.2914, 3.2779]])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rwp9IurDRBBT","executionInfo":{"status":"ok","timestamp":1646354099442,"user_tz":-180,"elapsed":5,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"6022e26b-d4fb-49e3-f366-0bef55ab247e"},"source":["print(x3.T)  #transpose operation"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.5970, 0.1905, 0.6418, 0.4970, 0.7858],\n","        [0.5432, 0.6017, 0.4654, 0.7959, 0.7914],\n","        [0.7493, 0.7007, 0.0508, 0.2184, 0.7779]])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwtpTOsXRBBU","executionInfo":{"status":"ok","timestamp":1646354099696,"user_tz":-180,"elapsed":259,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"03386477-ed52-457a-96b0-d1a11432c3ce"},"source":["print(x2.shape)    #shape method of the Tensor return dimentions of the tensor\n","print(x3.T.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 3])\n","torch.Size([3, 5])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVdzO7b8RBBU","executionInfo":{"status":"ok","timestamp":1646354099697,"user_tz":-180,"elapsed":4,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"d354e089-75d6-490c-ec45-d9b195683f5b"},"source":["print(x2 @ x3.T) # @ is matrix multiplication operator. You can use .matmul() method as well."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[4.7238, 3.7323, 2.8950, 3.7784, 5.8875],\n","        [4.7238, 3.7323, 2.8950, 3.7784, 5.8875],\n","        [4.7238, 3.7323, 2.8950, 3.7784, 5.8875],\n","        [4.7238, 3.7323, 2.8950, 3.7784, 5.8875],\n","        [4.7238, 3.7323, 2.8950, 3.7784, 5.8875]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"iWzFmbSBRBBV"},"source":["## Part 2 Gradient and Tensor"]},{"cell_type":"markdown","metadata":{"id":"HZAyG4vRRBBV"},"source":["\n","Pytorch's autograd module lets you take derivatives of the leaf nodes in computational graphs by calling `.backward()` method of the scalar tensor to be differentiated. In this example, derivative of `x` with respect to `out = mean(x + y)` is computed by autograd. Note that differentiation by a matrix (or tensor) is nothing but differentiating by all elements in the matrix.\n","\n","By default, Pytorch does not keep gradients of newly created tensors, unless you specify it when creating a tensor.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgHikPfuRsdF","executionInfo":{"status":"ok","timestamp":1646354116736,"user_tz":-180,"elapsed":236,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"da35ea74-765f-471e-d142-c7f3754ced0b"},"source":["x = torch.ones(2, 3, requires_grad=True)\n","y = torch.ones(2, 3, requires_grad=True) * 0.5\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], requires_grad=True)\n"]}]},{"cell_type":"markdown","metadata":{"id":"W7tP5YlwSSTK"},"source":["Pytorch also keeps track of the operations that create tensors to apply chain rule at backward propagation. For example, y is created by a multiplication while x is not."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VtQYN6A_SYaw","executionInfo":{"status":"ok","timestamp":1646354116978,"user_tz":-180,"elapsed":7,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"c1e9246e-8919-4978-e4b7-0ab12dfdea24"},"source":["print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.5000, 0.5000, 0.5000],\n","        [0.5000, 0.5000, 0.5000]], grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yeM7r8KeSdIP","executionInfo":{"status":"ok","timestamp":1646354116978,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"2d63f4eb-0145-423c-9fa1-a07133517022"},"source":["z = x + y\n","print(z)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.5000, 1.5000, 1.5000],\n","        [1.5000, 1.5000, 1.5000]], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-gdGEhpSi31","executionInfo":{"status":"ok","timestamp":1646354116979,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"4b5d35b9-0c4e-4a6d-aadf-3ef371a501f1"},"source":["out_scalar = z.mean()\n","print(out_scalar)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.5000, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"7rfb2UXGT3KT"},"source":["When you call `.backward()` of a 'scalar valued' tensor, autograd module automatically populates the `.grad` fields of the other tensors in computational graph. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdcPA5pKTxrZ","executionInfo":{"status":"ok","timestamp":1646354117492,"user_tz":-180,"elapsed":518,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"bfc85575-2873-4910-a004-838f4d24a979"},"source":["out_scalar.backward()\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1667, 0.1667, 0.1667],\n","        [0.1667, 0.1667, 0.1667]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"YomJu7STa_qd"},"source":["Following example taken from Lecture 3 Reading 1. In this example following graph  a simple calculation c*(a+2b) is calculated. First lets calculate our forward pass"]},{"cell_type":"markdown","metadata":{"id":"fTvrHGumYdyD"},"source":["![tensor example.png](data:image/png;base64,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)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdXE55rkYu8d","executionInfo":{"status":"ok","timestamp":1646354117492,"user_tz":-180,"elapsed":7,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"48e64dbf-0433-43c9-f843-4f4d81056405"},"source":["a = torch.tensor([3],requires_grad=True,dtype=float)\n","b = torch.tensor([1],requires_grad=True,dtype=float)\n","c = torch.tensor([-2.0],requires_grad=True,dtype=float)\n","print(f'a: {a} \\nb: {b} \\nc: {c}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a: tensor([3.], dtype=torch.float64, requires_grad=True) \n","b: tensor([1.], dtype=torch.float64, requires_grad=True) \n","c: tensor([-2.], dtype=torch.float64, requires_grad=True)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwnWMt1YZXQS","executionInfo":{"status":"ok","timestamp":1646354117492,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"343482bb-6fa7-42b3-b5ab-60a608b05320"},"source":["d = 2*b\n","d.retain_grad() # With this parameter we can keep gradients on non leaf nodes.\n","print(\"d: \",d)\n","e = a+d\n","e.retain_grad()\n","print(\"e: \",e)\n","L = c*e\n","L.retain_grad()\n","print(\"L: \",L)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["d:  tensor([2.], dtype=torch.float64, grad_fn=<MulBackward0>)\n","e:  tensor([5.], dtype=torch.float64, grad_fn=<AddBackward0>)\n","L:  tensor([-10.], dtype=torch.float64, grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"SHZuxWP9cKMk"},"source":["Lets run `.backwards` and see the result of our gradients"]},{"cell_type":"code","metadata":{"id":"nTTeMr_6ZtdH"},"source":["L.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-ticMRvaE9j","executionInfo":{"status":"ok","timestamp":1646354117493,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"78021711-3bda-4037-d5d6-364ed8849522"},"source":["print(\"gradient of e: \",e.grad) # Gradient of L with respect to e :  ∇(L)/∇(e) = c = -2\n","print(\"gradient of d: \",d.grad) #Gradient of L with respect to d  :  ∇(L)/∇(e)  * ∇(e)/∇(d) = -2* 1 =-2\n","print(\"gradient of c: \",c.grad) #Gradient of L with respect to c :  ∇(L)/∇(c) = e = 5\n","print(\"gradient of b: \",b.grad)  #Gradient of L with respect to d  :  ∇(L)/∇(e)  * ∇(e)/∇(d)  * ∇(d)/∇(b) = -2* 1 * 2 = -4\n","print(\"gradient of a: \",a.grad)  #Gradient of L with respect to d  :  ∇(L)/∇(e)  * ∇(e)/∇(a) = -2* 1 = -2"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["gradient of e:  tensor([-2.], dtype=torch.float64)\n","gradient of d:  tensor([-2.], dtype=torch.float64)\n","gradient of c:  tensor([5.], dtype=torch.float64)\n","gradient of b:  tensor([-4.], dtype=torch.float64)\n","gradient of a:  tensor([-2.], dtype=torch.float64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"OwvwBgudUAt7"},"source":["## Part 3 GPU and CUDA support"]},{"cell_type":"markdown","metadata":{"id":"UQ4A0kKpUT7n"},"source":["Pytorch supports CUDA backend for accelerating parallelizable operations such as matrix multiplication and convolution in GPU. In Colab, Pytorch is installed with CUDA support. You can check if cuda is avaliable in current session by calling `torch.cuda.is_avaliable()` method. If it returns `False`, you can enable it by choosing GPU in menu `Runtime -> Change runtime type -> Hardware Accelerator` and restarting the runtime."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_MpHzXKUTfe","executionInfo":{"status":"ok","timestamp":1646354126299,"user_tz":-180,"elapsed":230,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"f96832a6-1180-457f-cce1-35004c5962a8"},"source":["import torch\n","from time import time\n","\n","print(torch.cuda.is_available())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"markdown","metadata":{"id":"GbM0MC4yUi7o"},"source":["You can also check the GPU state with `nvidia-smi` in shell. You can invoke shell command with prefix `!` in the notebook."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6RYwV9YT-hg","executionInfo":{"status":"ok","timestamp":1646354126816,"user_tz":-180,"elapsed":289,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"ec689752-053d-4ce6-da96-f6d2cad9547c"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Mar  4 00:35:25 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8    27W / 149W |      3MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"LPOuTO47UraE"},"source":["You can send tensors in GPU memory by calling `.cuda()` methods from tensors. If you run an operations on tensors in the GPU memory, it will automatically be calculated in GPU so that you don't need to use CUDA for most operations. \n","\n","Here is a benchmark for multiplication of large matrices in CPU and GPU. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQW5yq-EUkvF","executionInfo":{"status":"ok","timestamp":1646354174057,"user_tz":-180,"elapsed":47242,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"8d0c025a-6311-47c8-8ba7-abefe686f195"},"source":["x = torch.rand(500, 700)\n","y = torch.rand(700, 900)\n","\n","start = time() \n","\n","for n in range(5000):# depending on the systems cpu this process may take some time\n","    x.matmul(y)\n","    \n","end = time()\n","\n","cpu_time = end - start\n","print(f'{cpu_time:.2f} seconds')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["47.23 seconds\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4MwmUR1Utbm","executionInfo":{"status":"ok","timestamp":1646354184702,"user_tz":-180,"elapsed":10649,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"f19bd4fb-8399-4f35-ab33-8b0dbfc48626"},"source":["x = torch.rand(500, 700).cuda()\n","y = torch.rand(700, 900).cuda()\n","\n","start = time()\n","\n","for n in range(5000):\n","    x.matmul(y)\n","    \n","end = time()\n","\n","gpu_time = end - start\n","print(f'{gpu_time:.2f} seconds')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.60 seconds\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFph-KGZU_UN","executionInfo":{"status":"ok","timestamp":1646354184702,"user_tz":-180,"elapsed":10,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"ad565e2a-63e5-44bf-db56-65b7d2ffb4db"},"source":["!df -h"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Filesystem      Size  Used Avail Use% Mounted on\n","overlay          79G   43G   37G  54% /\n","tmpfs            64M     0   64M   0% /dev\n","shm             5.7G     0  5.7G   0% /dev/shm\n","/dev/root       2.0G  1.2G  817M  59% /sbin/docker-init\n","/dev/sda1        86G   47G   40G  55% /opt/bin/.nvidia\n","tmpfs           6.4G   36K  6.4G   1% /var/colab\n","tmpfs           6.4G     0  6.4G   0% /proc/acpi\n","tmpfs           6.4G     0  6.4G   0% /proc/scsi\n","tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"]}]},{"cell_type":"markdown","metadata":{"id":"esle8dP_Vom2"},"source":["GPU had significant advantage here. Actually, this is not even a very significant difference when compared with how it speeds up deep neural networks."]},{"cell_type":"markdown","metadata":{"id":"bsMiu5tPVv-5"},"source":["##Part 4 Data Utilities"]},{"cell_type":"markdown","metadata":{"id":"v8vL5p7cWDoZ"},"source":["In addition to the scientific computing features of Pytorch, it has utilities for common operations in machine learning. These include dataset operations like sampling, shuffling, batching of data or loading of commonly used datasets. Some of these features are provided with `torchtext` library, which is a NLP specialized library which is officially supported by Pytorch team.  In this examplee, IMDB dataset is loaded using `torchtext.datasets`."]},{"cell_type":"code","metadata":{"id":"s3qokh7HWDQd","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1646354371051,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"dcc5ef38-1a03-4560-fb84-34bc9303b220"},"source":["import torch, torchtext\n","torchtext.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'0.11.0'"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"3TmgBCKod0tw"},"source":["Before we continue this part of the tutorial torchtext was updated to 0.9 version at 4 March  2021 alongside with PyTorch 1.8  update. This update changed how to process  the input data with torchtext fundamentally. Older version moved to `torchtext.legacy` branch. \n","\n","Why are we mentioning this? It is because many of the resources and tutorial on the internet created by older versions of the torchtext and Google Colab updated torchtext to latest version. In this part of the tutorial we will explain both versions of the torchtext."]},{"cell_type":"markdown","metadata":{"id":"iIh8OA-5XTH1"},"source":["### Torchtext.legacy"]},{"cell_type":"markdown","metadata":{"id":"ZZZ-7yMYXbgA"},"source":["Main concepts of TorchText.legacy is the `Field`. This parameter defines how the data should be processed."]},{"cell_type":"code","metadata":{"id":"_YNVkrcujnO1"},"source":["# set up fields\n","TEXT = torchtext.legacy.data.Field(tokenize='spacy', batch_first=True) \n","LABEL = torchtext.legacy.data.LabelField(sequential=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5g8e2jNk7j05"},"source":["Another  important module of `torchtext.legacy` is `datasets` which contains common datasets used in NLP.   \n","In this example we will load IMDB dataset using this module and  we will split into dataset in to train and test sets with help of its `split`  function .\n","Torchtext will automatically download the dataset from its repository if it is not cached. \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5IzXhiDboz6","executionInfo":{"status":"ok","timestamp":1646354510731,"user_tz":-180,"elapsed":132176,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"20e7c2db-3818-43f3-f12c-17bf74fa12b8"},"source":["# make splits for data\n","train, test = torchtext.legacy.datasets.IMDB.splits(TEXT, LABEL)\n","\n","print(\"train length is: \",len(train))\n","print(\"test length is: \",len(test))\n","print(vars(train[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading aclImdb_v1.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84.1M/84.1M [00:02<00:00, 31.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["train length is:  25000\n","test length is:  25000\n","{'text': ['Almost', 'the', 'entire', 'film', 'takes', 'place', 'in', 'a', 'public', 'bathhouse', 'in', 'China', '.', 'There', 'are', 'no', 'fancy', 'sets', ',', 'explosion', 'or', 'glamorous', 'people', '--', 'only', 'fine', 'writing', ',', 'acting', 'and', 'direction', '(', 'Hollywood', ',', 'take', 'note!).<br', '/><br', '/>An', 'estranged', 'son', 'returns', 'home', 'when', 'he', 'believes', 'his', 'father', 'is', 'dying', '.', 'He', 'is', 'surprised', 'to', 'see', 'that', 'Dad', 'looks', 'fine', 'and', 'is', 'going', 'about', 'running', 'the', 'family', 'business', 'as', 'usual', '.', 'In', 'fact', ',', 'he', 'notices', 'that', 'his', 'father', 'and', 'his', 'retarded', 'brother', 'have', 'really', 'forged', 'a', 'close', 'and', 'caring', 'relationship', 'and', 'it', 'soon', 'becomes', 'obvious', 'that', 'he', 'is', 'out', 'of', 'the', 'loop', '!', 'Dad', 'is', 'very', 'traditional', 'and', 'this', 'visiting', 'son', 'is', 'from', 'the', 'big', 'city', 'and', 'does', \"n't\", 'really', 'see', 'the', 'value', 'of', 'the', 'old', 'bathhouse', '.', 'How', 'their', 'relationship', 'changes', 'and', 'where', 'the', 'plot', 'goes', 'from', 'there', 'is', 'exceptional', 'and', 'believable.<br', '/><br', '/>I', 'was', 'happy', 'to', 'see', 'that', 'not', 'every', 'Chinese', 'movie', 'is', 'an', 'action', 'picture', '(', 'such', 'as', 'those', 'starring', 'Jet', 'Li', 'or', 'Crouching', 'Tiger', ',', 'Hidden', 'Dragon', ')', ',', 'as', 'I', 'do', \"n't\", 'particularly', 'care', 'for', 'these', 'frenetic', 'films', '.', 'The', 'Shower', 'as', 'well', 'as', 'Springtime', 'In', 'A', 'Small', 'Town', 'are', 'two', 'wonderful', 'examples', 'of', 'good', 'Chinese', 'films', 'about', 'PEOPLE', '!'], 'label': 'pos'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"Eeu4LAON_0RS"},"source":["Next task we need to do is  creating vocabulary.  Vocabulary is an important part of the process due to it will convert our string data to integers.  There are  two  main methods we can employ, which are character-level and word-level language models. Character level  models provide easy computation due to limit of characters. On the other hand Word level models are better at capturing semantic meanings.  \n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3Uop2Bne1aO","executionInfo":{"status":"ok","timestamp":1646354512346,"user_tz":-180,"elapsed":1623,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"4f82dc59-ba5b-4cd1-91fe-b08da846f38f"},"source":["# build the vocabulary\n","TEXT.build_vocab(train)\n","LABEL.build_vocab(train)\n","\n","print(\"Unique tokens in TEXT vocabulary:\",len(TEXT.vocab))\n","print(\"Unique tokens in LABEL vocabulary:\",len(LABEL.vocab))\n","print(TEXT.vocab.freqs.most_common(20))\n","print(LABEL.vocab.freqs)\n","print(TEXT.unk_token)\n","print(TEXT.pad_token)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique tokens in TEXT vocabulary: 121417\n","Unique tokens in LABEL vocabulary: 2\n","[('the', 289838), (',', 275296), ('.', 236843), ('and', 156483), ('a', 156282), ('of', 144055), ('to', 133886), ('is', 109095), ('in', 87676), ('I', 77546), ('it', 76545), ('that', 70355), ('\"', 63329), (\"'s\", 61928), ('this', 60483), ('-', 52863), ('/><br', 50935), ('was', 50013), ('as', 43508), ('with', 42807)]\n","Counter({'pos': 12500, 'neg': 12500})\n","<unk>\n","<pad>\n"]}]},{"cell_type":"markdown","metadata":{"id":"LHEtjH1ejJtT"},"source":["Following code utilizes `torch.device` module. Using this parameter we can send our tensors to GPU if it is available. "]},{"cell_type":"code","metadata":{"id":"J5d89eP2jGbs"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tOgTEtMwHeGp"},"source":["Last part of the preparing data is creating batches.\n","We will use `BuketIterator.splits` method to create train and test iterators. This function also works similar to `torch.utils.datadataloader`.  Different form ` dataloader`, `BuketIterator` pads the inputs according to maximum length corpus in the batch. In addition it tries to batch similar length examples to minimize the amount of padding. \n","\n"]},{"cell_type":"code","metadata":{"id":"Sg8rYkzMF4GR"},"source":["# make iterator for splits\n","train_iter, test_iter = torchtext.legacy.data.BucketIterator.splits(\n","    (train, test), batch_size=1, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qR43B7FuYUZd"},"source":["### Torchtext v0.10"]},{"cell_type":"markdown","metadata":{"id":"1Ib6N1JYYcGu"},"source":["In the newer version of the Torchtext data processing pipeline is changed. These changes explained in detail at [migration tutorial](https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb) given at [torchtext project page](https://pypi.org/project/torchtext/) \n","\n","Following code examples taken from migration tutorial.They show how to process data and create iterators using new version.\n","\n","One of the changes is removal of the `Field`. Because of this Datasets returns train and test data without preprocessing.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EfAEzea6cIXj","executionInfo":{"status":"ok","timestamp":1646354562640,"user_tz":-180,"elapsed":50299,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"aa6fd789-4e54-4d9e-9715-f0ad546e8ae4"},"source":["from torchtext.datasets import IMDB\n","train_iter, test_iter = IMDB(split=('train', 'test'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 84.1M/84.1M [00:02<00:00, 31.8MB/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"3g0dHjRAgL-C"},"source":["Data processing can be done using `torchtext.data.utils.get_tokenizer` function. Also instead of using `Field.build_vocab` function, `torchtext.vocab.Vocab` function can be used."]},{"cell_type":"code","metadata":{"id":"yO4fQLXfcZW_"},"source":["from torchtext.data.utils import get_tokenizer\n","tokenizer = get_tokenizer('spacy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ro8HXPwmwtp7"},"source":["# from collections import Counter\n","# from torchtext.vocab import Vocab\n","\n","# train_iter = IMDB(split='train')\n","# counter = Counter()\n","# for (label, line) in train_iter:\n","#     counter.update(tokenizer(line))\n","# vocab = Vocab(counter, min_freq=10, specials=('<unk>', '<PAD>'))\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","tokenizer = get_tokenizer('basic_english')\n","train_iter = IMDB(split='train')\n","\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6iosmFlh9FA","executionInfo":{"status":"ok","timestamp":1646354580216,"user_tz":-180,"elapsed":13,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"942a3886-0997-4f3b-ce76-c31dc9b485c5"},"source":["print(\"Unique tokens in TEXT vocabulary:\",len(vocab))\n","# print(vocab.freqs.most_common(5))\n","#print(vocab.get_parameter.freqs.most_common(5))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique tokens in TEXT vocabulary: 100683\n"]}]},{"cell_type":"markdown","metadata":{"id":"GJe-VdPyibqW"},"source":["Text transformation can be done by simple lambda function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ajf_M9m9cwYe","executionInfo":{"status":"ok","timestamp":1646354580217,"user_tz":-180,"elapsed":11,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"c4917a67-9d95-4dea-f5c6-001204498893"},"source":["text_transform = lambda x:  [vocab[token] for token in tokenizer(x)]\n","label_transform = lambda x: 1 if x == 'pos' else 0\n","\n","# Print out the output of text_transform\n","print(\"input to the text_transform:\", \"here is an example\")\n","print(\"output of the text_transform:\", text_transform(\"here is an example\"))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input to the text_transform: here is an example\n","output of the text_transform: [131, 9, 40, 464]\n"]}]},{"cell_type":"markdown","metadata":{"id":"y5jzwL9olDgG"},"source":["At last version of the torchtext `BucketIterator` is removed. Instead it is suggested that `DataLoader`  function of  pytorch to be used."]},{"cell_type":"code","metadata":{"id":"EC054Wlr0-xB"},"source":["from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","\n","def collate_batch(batch):\n","   label_list, text_list = [], []\n","   for (_label, _text) in batch:\n","        label_list.append(label_transform(_label))\n","        processed_text = torch.tensor(text_transform(_text))\n","        text_list.append(processed_text)\n","   return torch.tensor(label_list), pad_sequence(text_list, padding_value=3.0)\n","\n","train_iter = IMDB(split='train')\n","train_dataloader = DataLoader(list(train_iter), batch_size=8, shuffle=True, \n","                              collate_fn=collate_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTotRtXe1CWn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646354592423,"user_tz":-180,"elapsed":7,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"451f7d18-edf0-4a37-e805-b83519881b68"},"source":["# for idx, (label, text) in enumerate(train_dataloader):\n","#   model(item)\n","\n","# Or\n","next(iter(train_dataloader))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1, 1, 1, 1, 0, 0, 1, 1]),\n"," tensor([[  12,  366, 1884,  ...,  261,  874,   12],\n","         [  74,    7,    8,  ...,  298, 5060,  558],\n","         [  64, 2661,   15,  ...,   13,  398,  267],\n","         ...,\n","         [   3,    3,    3,  ...,  133,    3,    3],\n","         [   3,    3,    3,  ..., 1513,    3,    3],\n","         [   3,    3,    3,  ...,    2,    3,    3]]))"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"XCWHC2oLMyJB"},"source":["##Part 5 Optimization"]},{"cell_type":"markdown","metadata":{"id":"5iWFGhnpM36k"},"source":["Since most deep neural networks are trained with a Stochastic Gradient Descent (SGD) variant, Pytorch also has a module that provides most deep learning optimizers, and utilities like learning rate schedulers.\n"]},{"cell_type":"code","metadata":{"id":"Ufwhz3UA0tc0"},"source":["x = torch.tensor(100.0, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUIv3SXX0tc4"},"source":["y = torch.tensor(100.0, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydKuiVZ20tc6"},"source":["optimizer = torch.optim.SGD(params=[x, y],\n","                            lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjzqzg2DN7xD"},"source":["Before proceeding, I suggest you to find values of `x` and `y` to minimize `my_function(x, y)`"]},{"cell_type":"code","metadata":{"id":"B1yX1aL20tc9"},"source":["def my_function(x, y):\n","    return torch.square(x) + torch.square(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p0Bje6g8OAXz"},"source":["You can create optimizers by initializing the classes in `torch.optim` module with the parameters (needs to be an iterable like a list) to be optimized, and optimizer speceific hyperparameters such as learning rate and momentum.\n","Actually optimizers does not know anything about the optimized quantity. It can only access (and reset) the gradient fields (`.grad`) of the tensors in its `params=` argument.\n","\n","The `grad` field of tensors is populated by `.backward()` method of to-be-minimized tensor.\n","\n","In this example `x` and `y` is optimized to minimize `z` tensor. So `.backward()` needs is called from `z`. "]},{"cell_type":"code","metadata":{"id":"U1yIVm0ROGBz"},"source":["#This optimizer is going to optimize x and y\n","optimizer = torch.optim.SGD(params=[x, y],\n","                            lr=0.01)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9iGaY_1OJYR"},"source":["Now, try to run the cell below multiple times to see how SGD optimizer minimizes z by subtracting the gradients of x and y from themselves after multiplying gradients by the learning rate (lr).\n","\n","You can run cells before proceeding to the next cell by Ctrl+Enter"]},{"cell_type":"code","metadata":{"id":"tt3UbT8q0tdD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646354595792,"user_tz":-180,"elapsed":8,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"f0796a87-bc3d-4e63-97cf-082f0313a3fc"},"source":["#calculate x^2 + y^2\n","z = my_function(x, y)\n","\n","print(f'x before step is: {x.item()}')\n","print(f'y before step is: {y.item()}')\n","#calculate gradients of x and y wrt. z\n","optimizer.zero_grad()\n","z.backward()\n","\n","print(f'gradient of x is: {x.grad.item()}')\n","print(f'gradient of y is: {y.grad.item()}')\n","\n","#take a gradient descent step\n","optimizer.step()\n","\n","print(f'x^2 + y^2 = {z}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x before step is: 100.0\n","y before step is: 100.0\n","gradient of x is: 200.0\n","gradient of y is: 200.0\n","x^2 + y^2 = 20000.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"2ScCMexzOTx6"},"source":["##Part 6 Creating Neural Network"]},{"cell_type":"markdown","metadata":{"id":"rMkCX6qmOhNf"},"source":["To create neural networks with Pytorch, you need to create a Class that inherits `torch.nn.Module`. Then, you need to define the layers in the `__init__` method and define the forward propagation logic in `forward` method. This class can be considered as a blueprint of the network. It specifies the shapes of its parameters, but does not initialize the parameters. The example below shows a 3 layer network with input size 30 and layer sizes [20, 15, 1], respectively. Most common layers are provided by `torch.nn` module."]},{"cell_type":"code","metadata":{"id":"0EkH0L1_O1YF"},"source":["import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtzMcLlyOhNg"},"source":["class DumbNetwork(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layer1 = torch.nn.Linear(30, 20)\n","        self.layer2 = torch.nn.Linear(20, 15)\n","        self.layer3 = torch.nn.Linear(15, 1)\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = F.relu(x)\n","        x = self.layer2(x)\n","        x = F.relu(x)\n","        x = self.layer3(x)\n","        return x\n","        \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2N5X5vRTOhNh"},"source":["input_data = torch.randn(16, 30) #dummy input to match the network's input shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHkEcCPHOhNh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646354610660,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"6a885e88-31e9-4b89-ea03-f3bc4ea93e45"},"source":["print(input_data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 30])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Lq2zJ_RLOhNj"},"source":["To initialize and instantiate a network from the blueprint, you can just use the class initialization syntax of Python, which is shown below.\n","Note that if the `__init__` method takes arguments other than `self`, then you need to initialize the network with these arguments."]},{"cell_type":"code","metadata":{"id":"M8NmylK5OhNk"},"source":["network = DumbNetwork()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"im1xVcvZOhNl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646354610969,"user_tz":-180,"elapsed":313,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"a2f3790c-b3bb-4625-ea25-5a5544767608"},"source":["print(network)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DumbNetwork(\n","  (layer1): Linear(in_features=30, out_features=20, bias=True)\n","  (layer2): Linear(in_features=20, out_features=15, bias=True)\n","  (layer3): Linear(in_features=15, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"vfaG_fgIOhNm"},"source":["When the network is called via function call operator `()`, `forward` method of the network is called actually. Note that network handles inputs as batches and the batch dimension can be arbitrary."]},{"cell_type":"code","metadata":{"id":"Gi8ZNv4xOhNn"},"source":["output = network(input_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnvWNqYTOhNo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646354610971,"user_tz":-180,"elapsed":6,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"21307008-dc55-43b0-960e-e4b906e037e6"},"source":["print(output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"maTCl5jDIYJk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrjOdqKBvXBH"},"source":["## Part 7 Other useful libraries  "]},{"cell_type":"markdown","metadata":{"id":"jA2h3cazxVPt"},"source":["In natural language processing (NLP) domain, there are many useful libraries which provide preprocessing tools or complex pretrained models.In this part of the tutorial, we will talk about these libraries that we will be using during this semester. \n","\n","[`Gensim`](https://radimrehurek.com/gensim/index.html) is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.It provides many useful tools to create and train word vectors.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"lgZM7l6GvVpp","executionInfo":{"status":"ok","timestamp":1646354615565,"user_tz":-180,"elapsed":853,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"35c95fc9-dbd4-4af0-ce35-8c7d51c0a443"},"source":["import gensim \n","gensim.__version__ ## There is a 4.0 version available. Google Colab contains 3.6 version."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'3.6.0'"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"C-3TleVbwmuh"},"source":["[Natural Language Toolkit or NLTK]( https://www.nltk.org/) is an another library that is frequently used in language processing domain. This library contains many useful tools for tokenization, stemming, lemmatization and more. In the next tutorial we will create an NLP model using this library.\n","\n","[SpaCy]( https://spacy.io/) is another library  which provides similar utilities as NLTK. While NLTK focuses on research and teaching, spaCy is more focuses on  products and more refined pipelines."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSv2JZ6z86Ck","executionInfo":{"status":"ok","timestamp":1646354616608,"user_tz":-180,"elapsed":1052,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"d636323f-6e08-42ac-e20a-6fe0eef5646a"},"source":["import nltk\n","import spacy\n","print(nltk.__version__)  #There is a 3.5 version available. Google Colab contains 3.2.5 version.\n","print(spacy.__version__) # There is a 3.0 version available. Google Colab contains 2.2.4 version."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.2.5\n","2.2.4\n"]}]},{"cell_type":"markdown","metadata":{"id":"fx22Ive4_M6K"},"source":["And finally [huggingface]( https://huggingface.co/) libraries. These libraries provide the state of the art NLP models. Its [`transformers`]( https://huggingface.co/transformers/index.html)  library provides easy access to popular models like `BERT` and `GPT-2`. Because Colab does not contain this library, you need to install it first if you want to work on Colab.\n","\n","Please note that using newest versions of a library is not always necessary. Although newer versions can provide more functions and bugfixes, it can create dependency problems with other libraries in your system.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695},"id":"s7GnTTvcCGFP","executionInfo":{"status":"ok","timestamp":1646354627649,"user_tz":-180,"elapsed":11048,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"146e9099-f472-49bb-ac0c-8eaac5e1aff8"},"source":["!pip install transformers\n","import transformers\n","transformers.__version__"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 5.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 513 kB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 38.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 42.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'4.17.0'"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"50W1RgdU3P3j"},"source":["## Part 5 Recommended References\n","* For more information about tensors, you can refer to [pytorch tensor tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) or for more visual representations [tensorflow tensor tutorial](https://www.tensorflow.org/guide/tensor)\n","\n","* For more information about pytorch and its, you can refer to [pytorch tutorial page](https://pytorch.org/tutorials/index.html)"]},{"cell_type":"code","source":[""],"metadata":{"id":"1rUMpsGvy2CD"},"execution_count":null,"outputs":[]}]}