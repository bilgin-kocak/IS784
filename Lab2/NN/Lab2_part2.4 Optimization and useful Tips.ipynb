{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Optimization and useful Tips.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNHpRHkce/aEHcbpAW65xmm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["In this part of the tutorial, I will discuss some common mistakes I saw in previous year's assignments. For this purpose, I will talk about a few topics like;\n","  *  Big-O notation,\n","  * Tips about PyTorch\n","  * More complex topics like GPU optimization.\n","  \n","Also, I will add some links for you to follow if you want to check these topics more in-depth"],"metadata":{"id":"Hpcb09D12jKu"}},{"cell_type":"markdown","source":["# Big O notation\n","=============================\n","\n","Wikipedia explains Big-O notations as: \" a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.\" This means that it shows how long the computation will take in the worst-case scenario."],"metadata":{"id":"0ttMSRAeLZgl"}},{"cell_type":"code","source":["import time\n","def test (x):\n","  start =time.time()\n","  for i in range(x):\n","    for j in range (x):\n","        i*j\n","  stop =time.time()\n","  print(\"time\"+str(x)+\"is:\"+str(stop-start))"],"metadata":{"id":"4Q0ass47LYgS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we look at the simple example above, we can see that our `test` function calculates i*j x^2 times. When we increase x linearly, computation time will increase proportionally to x^2. We can say that this function has a complexity of O(n^2). In the example below, we gave x as 1000 2000 and 3000. When we increased x 2 times, computation increased nearly four times, and the original calculation increased nine times when we increased three times."],"metadata":{"id":"LfwoT0gMP6VN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"__oBI9zF2aml","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646737908878,"user_tz":-180,"elapsed":97831,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"b832c8e7-f65a-432b-95c2-1c178c7d71d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["time10000is:7.011929750442505\n","time20000is:27.872304677963257\n","time30000is:62.832411766052246\n"]}],"source":["test(10000)\n","test(20000)\n","test(30000)"]},{"cell_type":"markdown","source":["Then how is this going to help us in the NLP? Preprocessing steps generally require custom functions. One of the problems I encountered in previous semesters is that some students created preprocessing loops (tokenization etc.) inside each other. Although coding like this may not create problems while the dataset is small, It will increase your training times exponentially. Let's look at the IMDB dataset, for example. \n","* IMDB dataset is a relatively small dataset for sentiment analysis with 100.000 lines train and test set combined. Okay, `100.000` no problem, you probably dealt with image sets this big previously. \n","* Each line contains an average of `250` tokens up to 1800. Let's take the standard and say we have `2.5m` tokens in total. It is big, but we can handle it still. \n","* Each word has an average of `8` characters, which makes `20m` characters.\n","* The tricky part is that we want to remove punctuations from these characters. with `14` different  punctuation characters, it makes `280m` calculations to check whether they are punctuation or not\n","\n","Or we could remove punctuations after tokenization. It is reduced to `35m` calculations.\n","\n","Although it is a simple step to remove punctuations, adding one wrong step will increase the total number of calculations from 35m to 280m(i.e., an eight-fold time increase). "],"metadata":{"id":"HeBjeVVrQGrq"}},{"cell_type":"markdown","source":["You can check the following tutorial or other sources online for more information.\n","* https://www.geeksforgeeks.org/analysis-algorithms-big-o-analysis/"],"metadata":{"id":"-fvKIlqSZ8eZ"}},{"cell_type":"markdown","source":["# GPU and PyTorch\n","======================================\n","\n","If you recall from previous tutorials, PyTorch and other Deep learning libraries utilize Nvidia's CUDA API/language. CUDA helps these platforms use GPU at full potential. \n","\n","Although you can optimize your codes considering CPU optimization methods, you need to learn more about GPU architecture to optimize your codes for GPU. Although it is out of scope in this course, you can check classes given at the MMI department if you want to learn more about optimization for GPU.\n","\n","These are some of the things you need to consider while creating your codes:\n","\n","* Copying data from/to GPU takes lots of time. Try to avoid it as much as possible. (things like print (tensor) if tensor in Cuda)\n","\n","* Try to minimize operations. For example, use Torch.no grad or merge pointwise operations. In addition, bigger models do not always give better results, but they will increase your computation time.\n","\n","* Try to utilize GPU efficiently and optimize performance. The easiest way to use GPU effectively is to maximize your batch size. If your batch size is small, your model will utilize only a fraction of the GPU's resources. On the other hand, if you select your batch size too large, You could see a memory problem (You exceeded memory of the GPU due to model size* batch_size exceeded memory)"],"metadata":{"id":"xp5fOe08aXgO"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"dMoqqRygZ78Y","executionInfo":{"status":"ok","timestamp":1647583589263,"user_tz":-180,"elapsed":5991,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["torch.zeros(size=(1000,1000,1000);"],"metadata":{"id":"Frh0-Z6ijq5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.zeros(size=(1000,1000,1000,1000));# your colab will crash"],"metadata":{"id":"KXAw9U8nVIbe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can also reduce your model sizes utilizing different methods like pruning or quantization. For more information, you can check Machine Learning Systems Design and Deployment course given at the MMI department."],"metadata":{"id":"W--f8xoCkucP"}},{"cell_type":"markdown","source":["Recomended Tutorials\n","* [PyTorch optimization guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n","* [Multi GPU paralel computing guide](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)\n","* [tutorial on medium](https://medium.com/sicara/deep-learning-memory-usage-and-pytorch-optimization-tricks-e9cab0ead93)\n"],"metadata":{"id":"lwizLuTBNFi-"}},{"cell_type":"markdown","source":["# Weight Initialization\n","\n","========================================\n","\n","The last topic I want to talk about is weight initialization.. PyTorch initializes layer weights with uniform distribution by default, utilizing standard deviation as limits. But if you're going to change the default initialization, You can use [torch.nn.init](https://pytorch.org/docs/stable/nn.init.html) module to change the default initialization of your weights."],"metadata":{"id":"VUoA0QGdnOZe"}},{"cell_type":"markdown","source":["One of the initialization methods is Xavier initialization. This method initializes the layer so that variance between layers is constant. "],"metadata":{"id":"Or--SbosqfUU"}},{"cell_type":"code","source":["x = torch.empty(3,6)\n","torch.nn.init.xavier_normal_(x)"],"metadata":{"id":"h8RUYGqsktqq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647583628346,"user_tz":-180,"elapsed":275,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"1235be58-c4b3-4242-ffdc-3821f6a62687"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.3283, -0.1985,  0.6296, -0.3714,  0.0924, -0.7093],\n","        [-0.8120, -1.0073,  0.9944, -0.5294,  0.7641,  0.0185],\n","        [-0.5995,  0.0640, -0.6631,  0.1494, -0.4949,  0.2794]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Kaiming initialization, on the other hand, initializes the layer while considering the non-linearity of RELU layers."],"metadata":{"id":"cTYUDvj1sDj8"}},{"cell_type":"code","source":["torch.nn.init.kaiming_uniform_(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VlakVJpW0QbB","executionInfo":{"status":"ok","timestamp":1647583629368,"user_tz":-180,"elapsed":4,"user":{"displayName":"arif ozan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrYbbjhxiHU_Rc0pVMA2rbfoax0H85Mss6El4sQg=s64","userId":"12465436653524186566"}},"outputId":"e7fbf7d8-6915-41b5-a37c-b7afb398662a"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.7769,  0.1460, -0.7329,  0.9230,  0.2007, -0.9080],\n","        [ 0.6273,  0.8227,  0.7985, -0.5005, -0.3230, -0.3344],\n","        [-0.7048, -0.7225,  0.1837, -0.5248,  0.7227, -0.8803]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Okay, so we know how to change weights. The main question is why we need to change weights. Optimal initialization can prevent vanishing or exploding gradient problems. In addition, with different initialization methods, model can reach better equilibrium points.\n","You can olso check [Weight Initialization Techniques in Neural Networks](https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78) for further details"],"metadata":{"id":"Xdc_n9_rNDBI"}},{"cell_type":"code","source":[""],"metadata":{"id":"_jvjh8cOOdcZ"},"execution_count":null,"outputs":[]}]}